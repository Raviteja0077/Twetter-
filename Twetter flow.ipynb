{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0UVO8xw2HXgX"
      },
      "outputs": [],
      "source": [
        "def load_your_data():\n",
        "    \"\"\"Load your actual tweet emotion dataset\"\"\"\n",
        "    # Load from CSV file\n",
        "    df = pd.read_csv('your_tweet_data.csv')\n",
        "    tweets = df['text'].tolist()\n",
        "    emotions = df['emotion'].tolist()\n",
        "    return tweets, emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet Emotion Recognition with TensorFlow - Complete Working Code (FIXED)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# FIXED: Larger sample dataset (5 samples per class = 30 total)\n",
        "def create_sample_data():\n",
        "    \"\"\"Create sample tweet data with enough samples for stratified split\"\"\"\n",
        "    tweets = [\n",
        "        # Joy - 5 samples\n",
        "        \"I am so happy today!\", \"Life is wonderful\", \"Such joy and happiness\", \"Amazing day\", \"Feeling great\",\n",
        "        # Sadness - 5 samples\n",
        "        \"This is extremely sad\", \"Feeling gloomy and low\", \"This makes me cry\", \"So depressing\", \"Very upset\",\n",
        "        # Anger - 5 samples\n",
        "        \"I am furious about this\", \"This situation is enraging\", \"Can't stand it\", \"So mad\", \"Angry mood\",\n",
        "        # Surprise - 5 samples\n",
        "        \"Wow, what an amazing surprise!\", \"Didn't expect that\", \"What a shock\", \"Truly surprising\", \"Incredible!\",\n",
        "        # Fear - 5 samples\n",
        "        \"I feel scared and anxious\", \"Terrified of what's next\", \"This is frightening\", \"So scared\", \"Really scared\",\n",
        "        # Love - 5 samples\n",
        "        \"I absolutely love this amazing thing\", \"Such a loving heart\", \"Love is everywhere\", \"Pure love\", \"Feeling loved\"\n",
        "    ]\n",
        "\n",
        "    emotions = [\n",
        "        'joy', 'joy', 'joy', 'joy', 'joy',\n",
        "        'sadness', 'sadness', 'sadness', 'sadness', 'sadness',\n",
        "        'anger', 'anger', 'anger', 'anger', 'anger',\n",
        "        'surprise', 'surprise', 'surprise', 'surprise', 'surprise',\n",
        "        'fear', 'fear', 'fear', 'fear', 'fear',\n",
        "        'love', 'love', 'love', 'love', 'love'\n",
        "    ]\n",
        "\n",
        "    return tweets, emotions\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data(texts, labels, max_features=5000, max_len=50):\n",
        "    \"\"\"Preprocess text data and labels\"\"\"\n",
        "    # Tokenize texts\n",
        "    tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    X = tokenizer.texts_to_sequences(texts)\n",
        "    X = pad_sequences(X, maxlen=max_len)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(labels)\n",
        "    y = tf.keras.utils.to_categorical(y_encoded)\n",
        "\n",
        "    return X, y, tokenizer, label_encoder\n",
        "\n",
        "# Model creation function\n",
        "def create_model(max_features=5000, max_len=50, embed_dim=100, lstm_units=64, num_classes=6):\n",
        "    \"\"\"Create RNN model for emotion classification\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(max_features, embed_dim, input_length=max_len),\n",
        "        SpatialDropout1D(0.4),\n",
        "        LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prediction function\n",
        "def predict_emotion(model, tokenizer, label_encoder, text, max_len=50):\n",
        "    \"\"\"Predict emotion for new text\"\"\"\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len)\n",
        "    prediction = model.predict(padded, verbose=0)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    confidence = np.max(prediction)\n",
        "    emotion = label_encoder.inverse_transform([predicted_class])[0]\n",
        "    return emotion, confidence\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"=== Tweet Emotion Recognition with TensorFlow ===\\n\")\n",
        "\n",
        "    # Step 1: Load and prepare data\n",
        "    print(\"1. Loading and preprocessing data...\")\n",
        "    tweets, emotions = create_sample_data()\n",
        "    X, y, tokenizer, label_encoder = preprocess_data(tweets, emotions)\n",
        "\n",
        "    print(f\"   - Number of tweets: {len(tweets)}\")\n",
        "    print(f\"   - Number of emotion classes: {len(set(emotions))}\")\n",
        "    print(f\"   - Input shape: {X.shape}\")\n",
        "    print(f\"   - Output shape: {y.shape}\\n\")\n",
        "\n",
        "    # Step 2: Split data (FIXED: Use 25% test size which gives at least 6 test samples)\n",
        "    print(\"2. Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"   - Training samples: {len(X_train)}\")\n",
        "    print(f\"   - Test samples: {len(X_test)}\")\n",
        "\n",
        "    # Step 3: Create model\n",
        "    print(\"\\n3. Creating model...\")\n",
        "    model = create_model(num_classes=len(set(emotions)))\n",
        "    model.summary()\n",
        "\n",
        "    # Step 4: Train model\n",
        "    print(\"\\n4. Training model...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=20,\n",
        "        batch_size=4,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Step 5: Evaluate model\n",
        "    print(\"\\n5. Evaluating model...\")\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"   Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Step 6: Test predictions\n",
        "    print(\"\\n6. Testing predictions...\")\n",
        "    test_texts = [\n",
        "        \"I'm extremely happy about this news!\",\n",
        "        \"This makes me feel very sad\",\n",
        "        \"I'm so angry right now\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        emotion, confidence = predict_emotion(model, tokenizer, label_encoder, text)\n",
        "        print(f\"   Text: '{text}'\")\n",
        "        print(f\"   Predicted Emotion: {emotion} (Confidence: {confidence:.2f})\\n\")\n",
        "\n",
        "    # Save model\n",
        "    model.save('tweet_emotion_model.h5')\n",
        "    print(\"7. Model saved as 'tweet_emotion_model.h5'\")\n",
        "\n",
        "    return model, tokenizer, label_encoder\n",
        "\n",
        "# Run the program\n",
        "if __name__ == \"__main__\":\n",
        "    model, tokenizer, label_encoder = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-duFas2vIEGV",
        "outputId": "a9291fbd-ad7c-430d-8d18-c37998a455b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Tweet Emotion Recognition with TensorFlow ===\n",
            "\n",
            "1. Loading and preprocessing data...\n",
            "   - Number of tweets: 30\n",
            "   - Number of emotion classes: 6\n",
            "   - Input shape: (30, 50)\n",
            "   - Output shape: (30, 6)\n",
            "\n",
            "2. Splitting data...\n",
            "   - Training samples: 22\n",
            "   - Test samples: 8\n",
            "\n",
            "3. Creating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ spatial_dropout1d               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. Training model...\n",
            "Epoch 1/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 183ms/step - accuracy: 0.0700 - loss: 1.8006 - val_accuracy: 0.0000e+00 - val_loss: 1.7941\n",
            "Epoch 2/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.2966 - loss: 1.7840 - val_accuracy: 0.0000e+00 - val_loss: 1.7997\n",
            "Epoch 3/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3748 - loss: 1.7710 - val_accuracy: 0.1250 - val_loss: 1.8062\n",
            "Epoch 4/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.3218 - loss: 1.7671 - val_accuracy: 0.0000e+00 - val_loss: 1.8105\n",
            "Epoch 5/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.3752 - loss: 1.7690 - val_accuracy: 0.1250 - val_loss: 1.8138\n",
            "Epoch 6/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2838 - loss: 1.7550 - val_accuracy: 0.1250 - val_loss: 1.8202\n",
            "Epoch 7/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3860 - loss: 1.7363 - val_accuracy: 0.1250 - val_loss: 1.8245\n",
            "Epoch 8/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4585 - loss: 1.7111 - val_accuracy: 0.1250 - val_loss: 1.8275\n",
            "Epoch 9/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4750 - loss: 1.6977 - val_accuracy: 0.0000e+00 - val_loss: 1.8340\n",
            "Epoch 10/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5320 - loss: 1.6948 - val_accuracy: 0.1250 - val_loss: 1.8373\n",
            "Epoch 11/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.5123 - loss: 1.6812 - val_accuracy: 0.0000e+00 - val_loss: 1.8469\n",
            "Epoch 12/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.5674 - loss: 1.6651 - val_accuracy: 0.0000e+00 - val_loss: 1.8615\n",
            "Epoch 13/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7215 - loss: 1.6174 - val_accuracy: 0.1250 - val_loss: 1.8927\n",
            "Epoch 14/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.6609 - loss: 1.5635 - val_accuracy: 0.0000e+00 - val_loss: 1.9173\n",
            "Epoch 15/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 0.7048 - loss: 1.5911 - val_accuracy: 0.0000e+00 - val_loss: 1.8781\n",
            "Epoch 16/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7339 - loss: 1.4817 - val_accuracy: 0.0000e+00 - val_loss: 1.8660\n",
            "Epoch 17/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7267 - loss: 1.4589 - val_accuracy: 0.1250 - val_loss: 1.8596\n",
            "Epoch 18/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.7707 - loss: 1.4074 - val_accuracy: 0.1250 - val_loss: 1.8769\n",
            "Epoch 19/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.8879 - loss: 1.2802 - val_accuracy: 0.1250 - val_loss: 1.9440\n",
            "Epoch 20/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.6530 - loss: 1.2505 - val_accuracy: 0.1250 - val_loss: 1.9918\n",
            "\n",
            "5. Evaluating model...\n",
            "   Test Accuracy: 12.50%\n",
            "\n",
            "6. Testing predictions...\n",
            "   Text: 'I'm extremely happy about this news!'\n",
            "   Predicted Emotion: anger (Confidence: 0.35)\n",
            "\n",
            "   Text: 'This makes me feel very sad'\n",
            "   Predicted Emotion: sadness (Confidence: 0.28)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Text: 'I'm so angry right now'\n",
            "   Predicted Emotion: anger (Confidence: 0.25)\n",
            "\n",
            "7. Model saved as 'tweet_emotion_model.h5'\n"
          ]
        }
      ]
    }
  ]
}